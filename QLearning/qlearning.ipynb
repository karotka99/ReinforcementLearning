{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: gym-cliffwalking is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with svn+, git+, hg+, or bzr+).\n"
     ]
    }
   ],
   "source": [
    "pip install -e gym-cliffwalking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_cliffwalking\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('cliffwalking-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -1.0, False, {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -1.0, False, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous state: 0, Action: 3, Next state: 12\n",
      "Previous state: 12, Action: 0, Next state: 13\n",
      "Previous state: 13, Action: 0, Next state: 14\n",
      "Previous state: 14, Action: 0, Next state: 15\n",
      "Previous state: 15, Action: 0, Next state: 16\n",
      "Previous state: 16, Action: 0, Next state: 17\n",
      "Previous state: 17, Action: 0, Next state: 18\n",
      "Previous state: 18, Action: 0, Next state: 19\n",
      "Previous state: 19, Action: 0, Next state: 20\n",
      "Previous state: 20, Action: 0, Next state: 21\n",
      "Previous state: 21, Action: 0, Next state: 22\n",
      "Previous state: 22, Action: 0, Next state: 23\n",
      "Previous state: 23, Action: 1, Next state: 11\n"
     ]
    }
   ],
   "source": [
    "def egreedy_policy(q_values, state, epsilon):\n",
    "    # Get a random number from a uniform distribution between 0 and 1,\n",
    "    # if the number is lower than epsilon choose a random action\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(4)\n",
    "    # Else choose the action with the highest value\n",
    "    else:\n",
    "        return np.argmax(q_values[state])\n",
    "\n",
    "epsilon = 0.01\n",
    "num_states = 48\n",
    "num_actions = 4\n",
    "q_values = np.zeros((num_states, num_actions))\n",
    "alfa = 1\n",
    "gamma = 1\n",
    "\n",
    "# Iterate over 500 episodes\n",
    "for _ in range(10000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # While episode is not over\n",
    "    while not done:\n",
    "        # Choose action        \n",
    "        action = egreedy_policy(q_values, state, epsilon)\n",
    "        # Do the action\n",
    "        next_state, reward, done, test = env.step(action)\n",
    "        # Update q_values\n",
    "        td_target = reward + np.max(q_values[next_state])\n",
    "        td_error = td_target - gamma * q_values[state][action]\n",
    "        q_values[state][action] += alfa * td_error\n",
    "        if _ == 9999:\n",
    "            print(f\"Previous state: {state}, Action: {action}, Next state: {next_state}\")\n",
    "        # Update state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 11]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egreedy_policy(q_values, state, epsilon):  \n",
    "    # Get a random number from a uniform distribution between 0 and 1,\n",
    "    # if the number is lower than epsilon choose a random action\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(4)\n",
    "    # Else choose the action with the highest value\n",
    "    else:\n",
    "        return np.argmax(q_values[state])\n",
    "\n",
    "epsilon = 0.01\n",
    "num_states = 48\n",
    "num_actions = 4\n",
    "q_values = np.zeros((num_states, num_actions))\n",
    "alfa = 1\n",
    "gamma = 1\n",
    "\n",
    "\n",
    "# Iterate over 500 episodes\n",
    "for _ in range(1000):\n",
    "    state = env.reset()    \n",
    "    done = False\n",
    "    # While episode is not over\n",
    "    while not done:\n",
    "        # Choose action\n",
    "\n",
    "        action = egreedy_policy(q_values, state, epsilon)\n",
    "        # Do the action\n",
    "        next_state, reward, done, test = env.step(action)\n",
    "        # Update q_values\n",
    "        predict = q_values[state, action]\n",
    " \n",
    "        expected_q = 0\n",
    "        q_max = np.max(q_values[next_state, :])\n",
    "        greedy_actions = 0\n",
    "        for i in range(num_actions):\n",
    "            if q_values[next_state][i] == q_max:\n",
    "                greedy_actions += 1\n",
    "     \n",
    "        non_greedy_action_probability = epsilon / num_actions\n",
    "        greedy_action_probability = ((1 - epsilon) / greedy_actions) + non_greedy_action_probability\n",
    " \n",
    "        for i in range(num_actions):\n",
    "            if q_values[next_state][i] == q_max:\n",
    "                expected_q += q_values[next_state][i] * greedy_action_probability\n",
    "            else:\n",
    "                expected_q += q_values[next_state][i] * non_greedy_action_probability\n",
    "        td_target = reward + expected_q\n",
    "        td_error = td_target - gamma * q_values[state][action]\n",
    "        q_values[state][action] += alfa * td_error\n",
    "        if _ == 99999:\n",
    "            print(f\"Previous state: {state}, Action: {action}, Next state: {next_state}\")\n",
    "        # Update state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
